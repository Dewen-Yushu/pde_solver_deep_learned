{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.203485Z",
     "start_time": "2018-11-29T21:09:23.186984Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fix_layout(width:int=95):\n",
    "    from IPython.core.display import display, HTML\n",
    "    display(HTML('<style>.container { width:' + str(width) + '% !important; }</style>'))\n",
    "    \n",
    "fix_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- **the model represents H**\n",
    "- H is learned for one, and only one iteration. But the same H is used for every iteration.\n",
    "- H should satisfy $H0 = 0$\n",
    "- H is a circulant matrix (in theory...) so we should be able to expand or shrink it according to the required dimensions. This is not clear to me, H can be anything... **TODO** we need to check this. Otherwise I don't know how to *expand* to the test dimensions.\n",
    "- The layers are defined as Convolutional Layers only. With a kernel size of (3, 3), a strife of (1, 1), without any bias and linear activation (equal to no activation). See below.\n",
    "\n",
    "```\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "Conv2D(<filters>,\n",
    "       kernel_size=(3, 3), \n",
    "       strides=(1, 1), \n",
    "       use_bias=False,\n",
    "       activation='linear')\n",
    "```\n",
    "\n",
    "- The learning objective, or loss is the mean_square_error off the model being used in the iteration.\n",
    "- X is $u^k$ (given the constrains, and for some iteration $k$)\n",
    "- y is $u^*$\n",
    "\n",
    "T = some constant update matrix, c = some constant vector, $\\psi$ an iterator\n",
    "$$u^{k + 1} = \\psi(u^k) ) = T u^k + c$$\n",
    "$$w = \\psi(u) - u$$\n",
    "$$\\phi(u) = G(\\psi(u) + H w) = G(\\psi(u) + H(\\psi(u) - u))$$\n",
    "\n",
    "- But since the model tries to optimise the residuals... maybe the input of the model should be something different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.490388Z",
     "start_time": "2018-11-29T21:09:23.205389Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.linalg import circulant\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nnpde.functions import iterativeMethods as im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:17:36.353636Z",
     "start_time": "2018-11-29T21:17:36.344120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:17:09.062494Z",
     "start_time": "2018-11-29T21:17:09.053456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.501727Z",
     "start_time": "2018-11-29T21:09:23.492074Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "a = np.ones(N**2)\n",
    "b = -np.ones(N**2-1)*0.25\n",
    "c = -np.ones(N**2-N)*0.25\n",
    "\n",
    "A = np.diag(a) + np.diag(b, 1) + np.diag(b, -1) + np.diag(c, N) + np.diag(c, -N)\n",
    "\n",
    "\n",
    "b_top_idx = np.arange(N)\n",
    "b_bottom_idx = np.arange(N**2-N, N**2)\n",
    "b_left_idx = np.linspace(N, N**2-2*N, N-2, dtype = int)\n",
    "b_right_idx = np.linspace(2*N-1, N**2-N, N-2, dtype = int)\n",
    "\n",
    "b_idx = np.append(b_top_idx, b_bottom_idx)\n",
    "b_idx = np.append(b_idx, b_left_idx)\n",
    "b_idx = np.append(b_idx, b_right_idx)\n",
    "b = np.ones(np.shape(b_idx))*1\n",
    "f = np.zeros(N**2)\n",
    "\n",
    "u, res = im.jacobi(A, f, b_idx = b_idx, b = b, max_iters=200,tol = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.527505Z",
     "start_time": "2018-11-29T21:09:23.503613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6610, -0.6610, -0.6610],\n",
       "          [-0.6610, -0.6610, -0.6610],\n",
       "          [-0.6610, -0.6610, -0.6610]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # should be `nn.Conv2d(???, ???, 3, bias=False)` in our case\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3, bias=False) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "H = net(torch.ones(1, 1, 5, 5))\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:28:45.224549Z",
     "start_time": "2018-11-29T21:28:45.209884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000,  ..., -0.6610, -0.6610, -0.6610],\n",
       "        [-0.6610,  1.0000,  0.0000,  ..., -0.6610, -0.6610, -0.6610],\n",
       "        [-0.6610, -0.6610,  1.0000,  ..., -0.6610, -0.6610, -0.6610],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.6610,  1.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.6610, -0.6610,  1.0000]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reshape_H(H, N=N):\n",
    "    # Given our definition of the net we get a 3x3 matrix, or 9x1 vector.\n",
    "    # N is 10 (might of course change), therefor the shape of H should be N**2 x N**2\n",
    "    # As H is a circulant matrix (by design) we transform it to that shape:\n",
    "    #  - add 1 as the first element -> results in a Nx1 vector\n",
    "    #  - add 0 as the padding -> results in a N**2x1 vector\n",
    "    #  - rotate H (rot1) N times -> results in a N**2 N**2x1 vectors\n",
    "    #  - stack thoes new columns -> results in a N**2xN**2 matrix\n",
    "    dim = N**2\n",
    "    z = torch.zeros(dim) # pytorch can save the gradients... do not use the flag here, it will result in a \"leaf moved...\" error\n",
    "    z[0] = torch.ones(1)\n",
    "    z[1:N] = H.view(-1)\n",
    "    \n",
    "    idx_of_circulant = circulant(range(dim))\n",
    "\n",
    "    # no fucking clue if this is right... it will result in the correct shape\n",
    "    return torch.cat([z[idx_of_circulant[:, i]].view((dim, 1)) for i in range(dim)], dim=1)\n",
    "\n",
    "reshape_H(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:34:38.071880Z",
     "start_time": "2018-11-29T21:34:38.063974Z"
    }
   },
   "outputs": [],
   "source": [
    "T = torch.eye(N**2) - torch.from_numpy(A).float()\n",
    "G = torch.eye(N**2) \n",
    "\n",
    "def reset_boundaries(X, G=G):\n",
    "    # TODO \n",
    "    return X\n",
    "\n",
    "\n",
    "def iterator_step(H, T=T, G=G):\n",
    "    \"\"\"will define the matrix computation\"\"\"\n",
    "    return reset_boundaries(T + H@T - H, G=G)\n",
    "\n",
    "\n",
    "def apply_fn_n_times(fn, n, x):\n",
    "    # this is basically a fold-left, the iterator step will be applied n times: fn(fn(fn(... (fn(u0)))))\n",
    "    return F.reduce(lambda x_, _: fn(x_), range(n), x)\n",
    "\n",
    "    \n",
    "def solver_with_H(u0, k, H, T=T, G=G):\n",
    "    X = iterator_step(H, T=T, G=G)\n",
    "    return apply_fn_n_times(lambda u_: X.mm(u), k, u0)\n",
    "\n",
    "\n",
    "class CustLoss(nn.Module):\n",
    "    def __init__(self, u_stars, k=5):\n",
    "        super(CustLoss, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        # should be u0 and u*\n",
    "        # TODO `0` is the initial value, is this correct?\n",
    "        #self.u_stars = [(0, u_star) for u_star in u_stars]\n",
    "        self.u_stars = [(torch.zeros(N**2, 1), u_star) for u_star in u_stars]\n",
    "        \n",
    "    def forward(self, yPred, yTrue):\n",
    "        # we don't care about y, as this would be y_true??? or is it x???\n",
    "        \n",
    "        #H = torch.from_numpy(yPred).requires_grad(True)\n",
    "    \n",
    "        # since we doing for all u_stars we can do it probably in a better way (vectorised)\n",
    "        # TODO think about the aggregation of the loss\n",
    "        return torch.sum(torch.cat([torch.pow(solver_with_H(u0, k=self.k, H=reshape_H(yPred)) - u_star, 2) for u0, u_star in self.u_stars]))\n",
    "        #return torch.sum(torch.cat([torch.abs(solver_with_H(u0, k=self.k, H=reshape_H(yPred)) - u_star) for u0, u_star in self.u_stars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:38:20.984829Z",
     "start_time": "2018-11-29T21:38:20.902158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre backward prop Parameter containing:\n",
      "tensor([[[[ 0.1110,  0.3188,  0.2530],\n",
      "          [ 0.1454, -0.2697, -0.1683],\n",
      "          [-0.2099, -0.0387,  0.0524]]]], requires_grad=True)\n",
      "post backward prop Parameter containing:\n",
      "tensor([[[[ 0.1110,  0.3188,  0.2530],\n",
      "          [ 0.1454, -0.2697, -0.1683],\n",
      "          [-0.2099, -0.0387,  0.0524]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = CustLoss(Variable(torch.from_numpy(u.reshape(N, N)).float()))\n",
    "net = Net()\n",
    "H = net(torch.ones(1, 1, 5, 5)) # just ones as some initial values (can be anything but 0, can it? 0 for sure not...)\n",
    "ground_truth = torch.ones(1, 1, 5, 5) # since the output of the model will be H, which we don't know, so we don't care about the output\n",
    "\n",
    "loss = loss_fn(H, ground_truth)\n",
    "\n",
    "net.zero_grad() # H is the model\n",
    "print(\"pre backward prop\", net.conv1.weight)\n",
    "loss.backward()\n",
    "print(\"post backward prop\", net.conv1.weight)\n",
    "#optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:42:19.343657Z",
     "start_time": "2018-11-29T21:42:19.275623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post backward prop Parameter containing:\n",
      "tensor([[[[ 0.1110,  0.3188,  0.2530],\n",
      "          [ 0.1454, -0.2697, -0.1683],\n",
      "          [-0.2099, -0.0387,  0.0524]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "# well... as we can see nothing happens...\n",
    "H = net(torch.ones(1, 1, 5, 5)) # just ones as some initial values (can be anything but 0, can it? 0 for sure not...)\n",
    "ground_truth = torch.ones(1, 1, 5, 5) # since the output of the model will be H, which we don't know, so we don't care about the output\n",
    "optimizer.zero_grad()\n",
    "loss = loss_fn(H, ground_truth)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"post backward prop\", net.conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:34:49.209346Z",
     "start_time": "2018-11-29T21:34:49.203972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8958, -0.8958, -0.8958],\n",
       "          [-0.8958, -0.8958, -0.8958],\n",
       "          [-0.8958, -0.8958, -0.8958]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.ones(1, 1, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:34:51.044496Z",
     "start_time": "2018-11-29T21:34:51.038951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9771.2812, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.719516Z",
     "start_time": "2018-11-29T21:09:23.247Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = CustLoss(u)\n",
    "\n",
    "prediction = jacobi(A, b, H)\n",
    "loss = loss_fn(prediction, ground_truth)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "H.zero_grad() # H is the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.721037Z",
     "start_time": "2018-11-29T21:09:23.251Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.722090Z",
     "start_time": "2018-11-29T21:09:23.257Z"
    }
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.723065Z",
     "start_time": "2018-11-29T21:09:23.262Z"
    }
   },
   "outputs": [],
   "source": [
    "input = torch.ones(1, 1, 5, 5) # what to pluck here???\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.724086Z",
     "start_time": "2018-11-29T21:09:23.267Z"
    }
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T21:09:23.725067Z",
     "start_time": "2018-11-29T21:09:23.273Z"
    }
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
